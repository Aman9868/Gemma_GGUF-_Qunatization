{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd llama.cpp && LLAMA_CUBLAS=1 make && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir quantmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Model From HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "model_name=\"google/gemma-7b\"\n",
    "methods = ['q4_k_m'] # qunatizztaion method (4bit,8bit) medium m=medium s=small\n",
    "base_model=\"./original/\" # Download BASE Model inside current directory\n",
    "quant_path\"./mod/\n",
    "snapshot_download(repo_id=model_name, local_dir=base_model , local_dir_use_symlinks=False)\n",
    "org_model=quant_path+'/FP16.gguf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Quantization of Gemma Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python llama.cpp/convert-hf-to-gguf.py ./original_model/ --outtype f16 --outfile ./quantmodel/FP16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for i in methods:\n",
    "    qtype=f\"{quant_path}/{i.upper()}.gguf\"\n",
    "    os.system(\"./llama.cpp/quantize \"+quant_path+\"/FP16.gguf \"+qtype+\" \"+m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./llama.cpp/main -m ./quantmodel/Q4_K_M.gguf -n 90 --repeat_penalty 1.0 --color -i -r \"User:\" -f llama.cpp/prompts/chat-with-bob.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push Model to HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from huggingface_hub import HfApi, HfFolder, create_repo, upload_file\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='./quantmodel/Q4_K_M.gguf'\n",
    "repo_name = \"gema-7b-llm\"\n",
    "api=HfApi()\n",
    "api.upload_file(path_or_fileobj=model_path,\n",
    "    path_in_repo=\"Q4_K_M.gguf\",\n",
    "    repo_id=\"Amanaccessassist/gema-7b-llm\",\n",
    "    repo_type=\"model\",)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
